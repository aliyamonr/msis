{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaxylykbayeva/msis/blob/main/Text%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "49a6cbe6-f10c-4e80-b77d-47bde3848b8c",
        "_uuid": "f81f56104f6d5fc444447b41ec63aa369fd351a8",
        "id": "NBwn9uCPDWvu"
      },
      "source": [
        "# HW2: Text Classification\n",
        "In this homework, we will apply the Text Classification techniques to identify the author of the fictions. This dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. Your objective is to accurately identify the author of the sentences in the test set.\n",
        "\n",
        "The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer, so you may notice the odd non-sentence here and there. The problem requires us to predict the author, i.e. EAP, HPL and MWS given the text. In simpler words, text classification with 3 different classes. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d46ba3fd-26f1-4635-b2f9-fca916ff3066",
        "_uuid": "21f3ccd962d1556dc2346699d45a29e9ef791367",
        "id": "m9eXjBUgDWvv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "60326be1-82d1-4677-8ef8-da5b1eac475c",
        "_uuid": "adb496504ab8453ce2b4f91dd6e5f17cbdaf4f68",
        "id": "2e22PcBFDWvy"
      },
      "source": [
        "## Load data \n",
        "\n",
        "We first load the data and split the data into train and test. Note that do not modify the code in this cell. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "367e0329-7aeb-4f39-b1a9-d7395bdca993",
        "_uuid": "d6ea63db0ad0db09b25c35601391b71564601699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wNlfjvR8DWvz",
        "outputId": "cea1c75e-89fb-4820-e905-44bfb7edacc5"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/zariable/UW-MSIS541/master/assignments/hw1/data/data.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e171d134-bb33-4578-800c-2d65c2edf9c1",
        "_uuid": "c02f2a3e039aad543bc789188fe08422dd78f5c0",
        "id": "cKYvLrvsDWv1"
      },
      "source": [
        "We first preprocess the label by converting the label from string into integer. In particular, we use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c1ec2ba0-7bac-4983-8f68-850f26251eb6",
        "_uuid": "6b2ace3ea08492e59402dc2abcf65b99ff1a537e",
        "id": "yCoOoLuXDWv4"
      },
      "source": [
        "# label encode the author names into 0, 1 and 2 for easy evaluation.\n",
        "y = preprocessing.LabelEncoder().fit_transform(data.author.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "65403e74-091f-43c4-9523-3e15d8a75a1e",
        "_uuid": "4ffd04f40d9e921673d06ad64e01b9a7395d8e76",
        "id": "T_Bdv5OZDWwB"
      },
      "source": [
        "Then we randomly split the original data into train and test where training data is used to train the text classifier and test data is used to evaluate the model performance. We can do it using `train_test_split` from the `model_selection` module of scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ba8e606d-8dee-495e-8c3f-62aa916e9927",
        "_uuid": "b45676b121e2b719d355619e24cfed13d0d33f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "pnAGA9GYDWwC",
        "outputId": "10914bcc-d90b-494d-f09b-1fdc8d32f4a9"
      },
      "source": [
        "# split the data into train and test \n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data.text.values, y, \n",
        "    stratify=y, \n",
        "    random_state=42, \n",
        "    test_size=0.1, \n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "# print the first training example\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17621,)\n",
            "(1958,)\n",
            "Her hair was the brightest living gold, and despite the poverty of her clothing, seemed to set a crown of distinction on her head.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3db70c26-d684-478a-bcd4-980ed6c6d65b",
        "_uuid": "794fb768f4a8e42c4be4f1dbb27144aae4d00c79",
        "id": "FfZcjrp7DWwJ"
      },
      "source": [
        "## Task 1: Build a Naive Bayes Model \n",
        "\n",
        "Your very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a Naive Bayes classifier.\n",
        "\n",
        "Try to use Grid Search to find the best hyper parameter from the following settings (feel free to explore other options as well):\n",
        "\n",
        "* Differnet ngram range\n",
        "* Weather or not to remove the stop words\n",
        "* Weather or not to apply IDF\n",
        "\n",
        "I am intentionally make the requirement vague to encourage you to further explore different options and find the best solution. After identifying the best model, we use that model to make predictions on the test data and report its accuracy.  \n",
        "\n",
        "Hint: you can two options to extract TFIDF from the raw text. Option 1 is to follow the code we covered in the lab where we apply `CountVectorizer` followed by `TfidfTransformer`. Option 2 is to use the API `TfidfVectorizer`, which is equivalent to Option 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b387f2af-11b1-455d-ad8d-320ed1005be3",
        "_uuid": "350d453dc982f494c3774dbdcf731d856546d611",
        "id": "2gQttIRwDWwK"
      },
      "source": [
        "#TF-IDF\n",
        "count_vect = CountVectorizer(\n",
        "    stop_words = 'english',\n",
        "    max_features = None,\n",
        "    ngram_range = (1, 1)\n",
        ")\n",
        "\n",
        "X_train_counts = count_vect.fit_transform(x_train)\n",
        "X_train_counts.shape\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer(\n",
        "    norm = 'l2',\n",
        "    use_idf = True,\n",
        "    smooth_idf = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "YtevVeDZ76mm",
        "outputId": "43920d7a-6e95-4f64-e7a6-e551452f096e"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "text_clf = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer()), \n",
        "        ('tfidf', TfidfTransformer()), \n",
        "        ('clf', MultinomialNB())\n",
        "    ]\n",
        ")\n",
        "\n",
        "#Grid Search\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3), (1, 4)], 'tfidf__use_idf': (True, False), 'vect__stop_words': [\"english\", None]}\n",
        "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=1, cv=2)\n",
        "gs_clf = gs_clf.fit(x_train, y_train)\n",
        "\n",
        "print(gs_clf.best_params_)\n",
        "print(gs_clf.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tfidf__use_idf': True, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english'}\n",
            "0.7861072431517151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GpQDx9iRqdL"
      },
      "source": [
        "## Task 2: Build a Support Vector Machines (SVM) Model \n",
        "\n",
        "Similar to the first task, now you will build a SVM model for the same task. Use Grid Search to find the best hyper parameters and report the accuracy on the test from your best model.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "rQEjCxFDRr1V",
        "outputId": "9f1ff667-da76-4033-d135-e6ba69c43824"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "text_clf_svm = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer()), \n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf-svm', LinearSVC(random_state=0))\n",
        "    ]\n",
        ")\n",
        "\n",
        "text_clf_svm = text_clf_svm.fit(x_train, y_train)\n",
        "predicted_svm = text_clf_svm.predict(x_test)\n",
        "np.mean(predicted_svm == y_test)\n",
        "\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3), (1, 4)], 'tfidf__use_idf': (True, False), 'vect__stop_words': [\"english\", None]}\n",
        "\n",
        "gs_svm = GridSearchCV(text_clf_svm, parameters, n_jobs=1, cv=2)\n",
        "gs_svm = gs_svm.fit(x_train, y_train)\n",
        "\n",
        "print(gs_svm.best_params_)\n",
        "print(gs_svm.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tfidf__use_idf': True, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
            "0.8102831487984978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbG1anLzSMeH"
      },
      "source": [
        "## Task 3: Further improve the prediction accuracy\n",
        "\n",
        "Can you think of other ways to improve the prediction accuracy?\n",
        "\n",
        "* Can you further improve the prediction accuracy of Naive Bayes or SVM models?\n",
        "* Can you use Deep Neural Network to further improve model accuracy?\n",
        "* Can you apply different feature extraction?\n",
        "* Anything else you might think of to improve the prediction accuracy on the test data?\n",
        "\n",
        "Please write code to explore those options and report your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwvzjmdJSje-"
      },
      "source": [
        "###**Answer:**\n",
        "To further improve prediction accuracy, I decided to optimize more parameters of the feature extraction for both, Naive Bayes and SVM models in the Grid Search. \n",
        "\n",
        "In addition to the initial three parameters (N-gram range, usage of stop words, and usage of IDF), I added the following:\n",
        "\n",
        "\n",
        "> **For SVM:** an option to change the penalty term, to set fit_intercept to False, to solve the primal instead of the dual problem, and to flexibly set the amount of regularization.\n",
        "\n",
        "> **For Naive Bayes:** gave extra smoothing options\n",
        "\n",
        "For extra feature extraction, for both models, I applied sublinear tf scaling. This gives the model an extra option for interpreting word counts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "NVmgDSLnSP83",
        "outputId": "fa8461eb-57d4-4862-db4b-14fa8180dc7d"
      },
      "source": [
        "text_clf_svm2 = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer()), \n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf-svm', LinearSVC(random_state=0, max_iter=2000))\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'vect__stop_words': [\"english\", None],\n",
        "              'clf-svm__penalty': ['l1', 'l2'], 'clf-svm__fit_intercept':[True, False], 'clf-svm__dual':[False, True],\n",
        "              'tfidf__sublinear_tf':[True, False], 'clf-svm__C': [0.5, 1, 2]}\n",
        "\n",
        "gs_svm2 = GridSearchCV(text_clf_svm2, parameters, n_jobs=-1, cv=2)\n",
        "gs_svm2 = gs_svm2.fit(x_train, y_train)\n",
        "\n",
        "print(gs_svm2.best_params_)\n",
        "print(gs_svm2.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'clf-svm__C': 2, 'clf-svm__dual': True, 'clf-svm__fit_intercept': True, 'clf-svm__penalty': 'l2', 'tfidf__sublinear_tf': True, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
            "0.8129504691213169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "8geKbF7-GXlK",
        "outputId": "dbe5d6de-2fbd-4248-9ad7-2b69d48709a0"
      },
      "source": [
        "text_clf2 = Pipeline(\n",
        "    [\n",
        "        ('vect', CountVectorizer()), \n",
        "        ('tfidf', TfidfTransformer()), \n",
        "        ('clf', MultinomialNB())\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'vect__stop_words': [\"english\", None],\n",
        "              'clf__alpha': [0, 0.5, 1, 2], 'tfidf__sublinear_tf':[True, False]}\n",
        "\n",
        "gs_clf2 = GridSearchCV(text_clf2, parameters, n_jobs=-1, cv=2)\n",
        "gs_clf2 = gs_clf2.fit(x_train, y_train)\n",
        "\n",
        "print(gs_clf2.best_params_)\n",
        "\n",
        "print(gs_clf2.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'clf__alpha': 0.5, 'tfidf__sublinear_tf': True, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1), 'vect__stop_words': None}\n",
            "0.8077858125697022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egJhdsO5T3m7"
      },
      "source": [
        "###**Observations:**\n",
        "\n",
        "As a result of my modifications to the initial Grid Search, SVM model prediction accuracy has improved by **0.27%** (81.03% vs. 81.30%), and Naive Bayes model prediction accuracy improved by **2.17%** (78.61% vs. 80.78%)."
      ]
    }
  ]
}